{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"co_attention_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mginZGe4lt3C"},"source":["# Data preparation"]},{"cell_type":"code","metadata":{"id":"nbWLUxZwZV6l"},"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","random.seed(SEED)\n","\n","test_name = 'co_attention_1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"656CZGn7VtVE","executionInfo":{"status":"ok","timestamp":1610574950375,"user_tz":-60,"elapsed":23207,"user":{"displayName":"eknid mucollari","photoUrl":"","userId":"17739169050534207393"}},"outputId":"fd3ddb15-645e-48b8-eeef-81f8f5937df9"},"source":["# Set the base directory for Colab and non Colab environment\n","if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  base_dir = '/content/drive/My Drive/AN2DL/homework_3' \n","else:\n","  base_dir = os.getcwd()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0JhGwTm5lmCe"},"source":["dataset_dir = 'VQA_Dataset'\n","if not os.path.exists(dataset_dir):\n","    !unzip '/content/drive/MyDrive/AN2DL/homework_3/anndl-2020-vqa.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGMuBoZ39Hn-"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Create training ImageDataGenerator object for images\n","train_img_data_gen = ImageDataGenerator(rotation_range=10,\n","                                        width_shift_range=10,\n","                                        height_shift_range=10,\n","                                        zoom_range=0.3,\n","                                        horizontal_flip=True,\n","                                        vertical_flip=True,\n","                                        fill_mode='nearest',\n","                                        rescale=1./255)\n","\n","# Create validation ImageDataGenerator object\n","valid_img_data_gen = ImageDataGenerator(rescale=1./255)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSTQgaa5aRdp"},"source":["import json\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Load the dictionary containing the references to files\n","dataset_filepath = os.path.join(dataset_dir, 'train_questions_annotations.json')\n","with open(dataset_filepath, 'r') as f:\n","  dataset_file = json.load(f)\n","\n","# Splitting training and validation\n","training_count = int(0.8 * len(dataset_file))\n","validation_count = len(dataset_file) - training_count\n","\n","items = list(map(lambda x: x[1], dataset_file.items()))\n","random.shuffle(items)\n","\n","training_items = items[:training_count]\n","validation_items = items[-validation_count:]\n","\n","# Creating Tokenizer\n","tokenizer = Tokenizer()\n","\n","# Fit tokenizer on training questions\n","train_questions = list(map(lambda x: x['question'], training_items))\n","tokenizer.fit_on_texts(train_questions)\n","\n","# Using tokenizer to tokenize training questions\n","tokenized_train_questions = tokenizer.texts_to_sequences(train_questions)\n","max_question_length = max(len(question) for question in tokenized_train_questions)\n","question_inputs_train = pad_sequences(tokenized_train_questions, maxlen=max_question_length, padding='post')\n","\n","# Using tokenizer to tokenize validation questions\n","valid_questions = list(map(lambda x: x['question'], validation_items))\n","tokenized_valid_questions = tokenizer.texts_to_sequences(valid_questions)\n","question_inputs_valid = pad_sequences(tokenized_valid_questions, maxlen=max_question_length, padding='post')\n","\n","# Setting dimension of the dictionary\n","dictionary_dim = len(tokenizer.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7mWSFGRZdNG"},"source":["from PIL import Image\n","\n","# Define a custom dataset class extending Keras Sequence\n","class CustomDataset(tf.keras.utils.Sequence):\n","  \"\"\"\n","    CustomDataset inheriting from tf.keras.utils.Sequence.\n","\n","    3 main methods:\n","      - __init__: save dataset params\n","      - __len__: return the total number of samples in the dataset\n","      - __getitem__: return a sample from the dataset\n","  \"\"\"\n","\n","\n","  LABELS_DICT = {\n","    '0': 0,\n","    '1': 1,\n","    '2': 2,\n","    '3': 3,\n","    '4': 4,\n","    '5': 5,\n","    'apple': 6,\n","    'baseball': 7,\n","    'bench': 8,\n","    'bike': 9,\n","    'bird': 10,\n","    'black': 11,\n","    'blanket': 12,\n","    'blue': 13,\n","    'bone': 14,\n","    'book': 15,\n","    'boy': 16,\n","    'brown': 17,\n","    'cat': 18,\n","    'chair': 19,\n","    'couch': 20,\n","    'dog': 21,\n","    'floor': 22,\n","    'food': 23,\n","    'football': 24,\n","    'girl': 25,\n","    'grass': 26,\n","    'gray': 27,\n","    'green': 28,\n","    'left': 29,\n","    'log': 30,\n","    'man': 31,\n","    'monkey bars': 32,\n","    'no': 33,\n","    'nothing': 34,\n","    'orange': 35,\n","    'pie': 36,\n","    'plant': 37,\n","    'playing': 38,\n","    'red': 39,\n","    'right': 40,\n","    'rug': 41,\n","    'sandbox': 42,\n","    'sitting': 43,\n","    'sleeping': 44,\n","    'soccer': 45,\n","    'squirrel': 46,\n","    'standing': 47,\n","    'stool': 48,\n","    'sunny': 49,\n","    'table': 50,\n","    'tree': 51,\n","    'watermelon': 52,\n","    'white': 53,\n","    'wine': 54,\n","    'woman': 55,\n","    'yellow': 56,\n","    'yes': 57\n","  }\n","\n","\n","  def __init__(self, items, question_inputs=None, max_question_length=None, img_generator=None, out_shape=None):\n","    \"\"\"\n","      Initialize the object.\n","\n","      Keyword arguments:\n","      which_subset -- 'training' for the training set, else 'validation'\n","      tokenizer -- tokenizer object for the 'validatio' set, in case of 'training' the object will be created\n","      max_question_length -- max length of a question of the training set (used for tokenizing 'validation'), in case of 'training' the parameter will be calculated\n","      img_generator -- ImageDataGenerator objet to apply to the images or None\n","      out_shape -- output shape for the images, a tuple (height, width) or None for original size\n","    \"\"\"\n","    \n","    # Set class properties\n","    self.items = items\n","    self.question_inputs = question_inputs\n","    self.max_question_length = max_question_length\n","    self.img_generator = img_generator\n","    self.out_shape = out_shape\n","\n","  def __len__(self):\n","    \"\"\"\n","      Return the length of the dataset.\n","    \"\"\"\n","    return len(self.items)\n","\n","  def __getitem__(self, index):\n","    \"\"\"\n","      Return an item from the set.\n","\n","      Keyword arguments:\n","      index -- index of the item to return\n","    \"\"\"\n","\n","    # Read Image\n","    curr_item = self.items[index]\n","    img_id = curr_item['image_id']\n","    img = Image.open(os.path.join(dataset_dir, 'Images', img_id + '.png'))\n","\n","    # Convert image from RGBA to RGB\n","    img = img.convert('RGB')\n","\n","    # Resize image\n","    if self.out_shape is not None:\n","      img = img.resize(self.out_shape)\n","\n","    img_arr = np.array(img)\n","\n","    if self.img_generator is not None:\n","      # Perform data augmentation\n","      # Get a random transformation from the ImageDataGenerator and we can apply it to the image\n","      img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n","      img_arr = self.img_generator.apply_transform(img_arr, img_t)\n","\n","    # Convert answer to one-hot\n","    answer = np.zeros(len(self.LABELS_DICT))\n","    answer[self.LABELS_DICT[curr_item['answer']]] = 1\n","\n","    return (self.question_inputs[index], np.float32(img_arr)), answer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QolQ3xviAaGy"},"source":["# Set the sizes for the reshaped images\n","img_w = 700\n","img_h = 400\n","\n","# Set the number of classes\n","num_classes = 58\n","\n","# Set batch size\n","bs = 16\n","\n","# Create training and validation set generators\n","dataset = CustomDataset(training_items, question_inputs_train, max_question_length, img_generator=train_img_data_gen, out_shape=(img_w, img_h))\n","dataset_valid = CustomDataset(validation_items, question_inputs_valid, max_question_length, img_generator=valid_img_data_gen, out_shape=(img_w, img_h))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rk14AY2SBVdf"},"source":["# Create training data set from the generator\n","train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n","                                               output_types=((np.int32, np.float32), np.int32),\n","                                               output_shapes=(([dataset.max_question_length,], [img_h, img_w, 3]), (num_classes)))\n","train_dataset = train_dataset.batch(bs)\n","\n","train_dataset = train_dataset.repeat()\n","\n","# Create validation data set from the generator\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=((np.int32, np.float32), np.int32),\n","                                               output_shapes=(([dataset.max_question_length,], [img_h, img_w, 3]), (num_classes)))\n","valid_dataset = valid_dataset.batch(1)\n","\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fs8X5Op17wyX"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"Kh3Zgqq9snNa"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8):\n","        super(MultiHeadAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        if embed_dim % num_heads != 0:\n","            raise ValueError(\n","                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n","            )\n","        self.projection_dim = embed_dim // num_heads\n","        self.query_dense = tf.keras.layers.Dense(embed_dim)\n","        self.key_dense = tf.keras.layers.Dense(embed_dim)\n","        self.value_dense = tf.keras.layers.Dense(embed_dim)\n","        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output, weights\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, query, key, value):\n","        # x.shape = [batch_size, seq_len, embedding_dim]\n","        batch_size = tf.shape(query)[0]\n","\n","        # (batch_size, seq_len, embed_dim)\n","        query = self.query_dense(query)  \n","        key = self.key_dense(key)  \n","        value = self.value_dense(value)\n","\n","        query = self.separate_heads(\n","            query, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        key = self.separate_heads(\n","            key, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        value = self.separate_heads(\n","            value, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        attention, weights = self.attention(query, key, value)\n","        attention = tf.transpose(\n","            attention, perm=[0, 2, 1, 3]\n","        )  # (batch_size, seq_len, num_heads, projection_dim)\n","        concat_attention = tf.reshape(\n","            attention, (batch_size, -1, self.embed_dim)\n","        )  # (batch_size, seq_len, embed_dim)\n","        output = self.combine_heads(\n","            concat_attention\n","        )  # (batch_size, seq_len, embed_dim)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmI5p4sHbm3S"},"source":["class TransformerEncoder(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        self.att = MultiHeadAttention(embed_dim, num_heads)\n","        self.ffn = tf.keras.Sequential(\n","            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        if len(inputs) == 3:\n","            attn_output = self.att(inputs[0], inputs[1], inputs[2])\n","        elif len(inputs) == 2:\n","            attn_output = self.att(inputs[0], inputs[1], inputs[1])\n","        elif len(inputs) == 1:\n","            attn_output = self.att(inputs[0], inputs[0], inputs[0])\n","        else:\n","            raise ValueError(\n","                \"encoder's input list length must be 1, 2 or 3\"\n","            )\n","\n","\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs[0] + attn_output)\n","        \n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-qZMXgLDGsh"},"source":["class TransformerDecoder(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerDecoder, self).__init__()\n","        self.att_s = MultiHeadAttention(embed_dim, num_heads)\n","        self.att_g = MultiHeadAttention(embed_dim, num_heads)\n","        self.ffn = tf.keras.Sequential(\n","            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_s = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm_g = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm_o = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout_s = tf.keras.layers.Dropout(rate)\n","        self.dropout_g = tf.keras.layers.Dropout(rate)\n","        self.dropout_o = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        q = inputs[0]\n","        if len(inputs) == 2:\n","            k = inputs[1] \n","            v = inputs[1]\n","        elif len(inputs) == 3:\n","            k = inputs[1] \n","            v = inputs[2]\n","        else:\n","            raise ValueError(\n","                \"decoder's input list length must be 2 or 3\"\n","            )\n","\n","        # Self attention\n","        attn_output = self.att_s(q, q, q)\n","        attn_output = self.dropout_s(attn_output, training=training)\n","        q = self.layernorm_s(q + attn_output)\n","\n","        # Guided Attention\n","        attn_output = self.att_g(q, k, v)\n","        attn_output = self.dropout_g(attn_output, training=training)\n","        q = self.layernorm_g(q + attn_output)       \n","       \n","        ffn_output = self.ffn(q)\n","        ffn_output = self.dropout_o(ffn_output, training=training)\n","        return self.layernorm_o(q + ffn_output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTTw8rirmctn"},"source":["embeddings_index = {}\n","f = open(os.path.join(base_dir, 'glove.6B', 'glove.6B.300d.txt'))\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RDj_987wmehd"},"source":["embedding_matrix = np.zeros((dictionary_dim + 1, 300))\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WsmEwRGvbwyA"},"source":["class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim, embedding_matrix):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size,\n","                                                   output_dim=embed_dim,\n","                                                   weights=[embedding_matrix],\n","                                                   trainable=False)\n","        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETUGwBrU7mgz"},"source":["def conv2D(x, filters, k_size=3, strirdes=1, relu=0, pool_size=0):\n","    x = tf.keras.layers.Conv2D(filters=filters, \n","                               kernel_size=k_size,\n","                               strides=strirdes,\n","                               padding='same',\n","                               kernel_regularizer='l2',\n","                               input_shape=[None])(x)\n","    if relu:\n","        x = tf.keras.layers.ReLU()(x)\n","    if pool_size:\n","        x = tf.keras.layers.MaxPool2D(pool_size=pool_size)(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fryna4BvgIWR"},"source":["def vision_embending(inputs, out_length, question_length):\n","    x = tf.keras.applications.DenseNet121(input_tensor=inputs, include_top=False, weights='imagenet').output\n","    output = []\n","\n","    for i in range(question_length):\n","        y = conv2D(x, out_length, k_size=1, strirdes=1, relu=1)\n","        y = tf.keras.layers.GlobalAveragePooling2D()(y)\n","        output.append(y)\n","    \n","    x = tf.keras.layers.Concatenate()(output)\n","\n","    return tf.keras.layers.Reshape((question_length, out_length))(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"8LUNR7b07wyh","executionInfo":{"status":"error","timestamp":1611793190221,"user_tz":-60,"elapsed":817,"user":{"displayName":"eknid mucollari","photoUrl":"","userId":"17739169050534207393"}},"outputId":"3876dbfe-bb0d-4df5-b68e-d107abfd84bb"},"source":["# Free up RAM in case the model definition cells were run multiple times\n","tf.keras.backend.clear_session()\n","\n","# Define Transformer for language input\n","input_length = max_question_length\n","vocab_size = dictionary_dim + 1\n","embed_dim = 300  # Embedding size for each token\n","num_heads = 8  # Number of attention heads\n","encode_dim = 512  # Encoding size for each encoder\n","ff_dim = 512  # Hidden layer size in feed forward network inside transformer\n","\n","################################### Immage Embenddign ##############################################\n","image_input = tf.keras.layers.Input(shape=(img_h, img_w, 3))\n","image_embedding = vision_embending(image_input, 512, input_length)\n","\n","################################### Question Embedding #############################################\n","question_input = tf.keras.layers.Input(shape=(input_length), dtype='int32')\n","#question_embedding = TokenAndPositionEmbedding(input_length, vocab_size, embed_dim, embedding_matrix)(question_input)\n","question_embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n","                                               output_dim=embed_dim,\n","                                               weights=[embedding_matrix])(question_input)\n","question_embedding = tf.keras.layers.LSTM(encode_dim, return_sequences=True)(question_embedding)\n","\n","################################## Encoder #########################################################\n","# 1 SAtt\n","enc = TransformerEncoder(encode_dim, num_heads, ff_dim)([question_embedding])\n","# 2 SAtt\n","enc = TransformerEncoder(encode_dim, num_heads, ff_dim)([enc]) \n","# 3 SAtt \n","enc = TransformerEncoder(encode_dim, num_heads, ff_dim)([enc])\n","\n","################################## Decoder #########################################################\n","# 1 SGAtt\n","dec = TransformerEncoder(encode_dim, num_heads, ff_dim)([image_embedding, enc])\n","# 2 SGAtt\n","dec = TransformerEncoder(encode_dim, num_heads, ff_dim)([dec, enc])\n","# 3 SGAtt\n","dec = TransformerEncoder(encode_dim, num_heads, ff_dim)([dec, enc])\n","\n","################################### Feature fusion #################################################\n","# Question\n","enc_alpha = tf.keras.layers.Dense(encode_dim, activation='relu')(enc)\n","enc_alpha = tf.keras.layers.Dropout(0.1)(enc_alpha)\n","enc_alpha = tf.keras.layers.Dense(1, activation='softmax')(enc_alpha)\n","\n","question_out = tf.keras.layers.multiply([enc_alpha, enc])\n","question_out = tf.reduce_sum(question_out, 1)\n","question_out = tf.keras.layers.Dense(encode_dim, activation='relu')(question_out)\n","\n","# Image\n","dec_alpha = tf.keras.layers.Dense(encode_dim, activation='relu')(dec)\n","dec_alpha = tf.keras.layers.Dropout(0.1)(dec_alpha)\n","dec_alpha = tf.keras.layers.Dense(1, activation='softmax')(dec_alpha)\n","\n","image_out = tf.keras.layers.multiply([dec_alpha, dec])\n","image_out = tf.reduce_sum(image_out, 1)\n","image_out = tf.keras.layers.Dense(encode_dim, activation='relu')(image_out)\n","\n","################################### Merging and output #############################################\n","merged = tf.keras.layers.LayerNormalization()(question_out + image_out)\n","output = tf.keras.layers.Dense(units=num_classes, activation='softmax')(merged)\n","model = tf.keras.models.Model(inputs=[question_input, image_input], outputs=output)\n","\n","# Visualize created model\n","model.summary()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cece1323e117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Free up RAM in case the model definition cells were run multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define Transformer for language input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_question_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"iiXyQ7dgovJe"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"KnnXNjSal_en"},"source":["# Loss function\n","loss = tf.keras.losses.CategoricalCrossentropy() \n","\n","# Learning rate and oprimizer\n","lr = 1e-3\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","# Number of epochs\n","epochs = 100\n","\n","# Validation metrics\n","metrics = ['accuracy']\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ck6esfPtmNYP"},"source":["exp_dir = os.path.join(base_dir, test_name)\n","if not os.path.exists(exp_dir):\n","  os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Checkpoint callback, generate a checkpoint at each epoch\n","# There is only one checkpoint overwritten only if the new one has an lower validation loss\n","ckpt_dir = os.path.join(exp_dir, 'checkpoints')\n","if not os.path.exists(ckpt_dir):\n","  os.makedirs(ckpt_dir)\n","checkpoint = os.path.join(ckpt_dir, 'checkpoint_' + test_name + '.ckpt')\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint,\n","                                                   save_weights_only=True,\n","                                                   monitor='val_loss',\n","                                                   mode='min',\n","                                                   save_best_only=True)\n","callbacks.append(ckpt_callback)\n","\n","# Callback for tensorboard logs\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","  os.makedirs(tb_dir)\n","    \n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)\n","callbacks.append(tb_callback)\n","\n","\n","# Callback for early stopping in order to optimize the number of epochs\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5)\n","callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPkEUjArmH4a"},"source":["#%load_ext tensorboard\n","#%tensorboard --logdir '$base_dir'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"04N_zH9Xmzdn","colab":{"base_uri":"https://localhost:8080/","height":556},"executionInfo":{"status":"error","timestamp":1610456892314,"user_tz":-60,"elapsed":11933026,"user":{"displayName":"eknid mucollari","photoUrl":"","userId":"17739169050534207393"}},"outputId":"4935be9f-7d10-4529-f003-beca7535854e"},"source":["# If there exists a checkpoint it's loaded, otherwise the model is trained\n","# If train is True and there exists a checkpoint the trainig continues from it\n","\n","train = True\n","load = False\n","\n","if os.path.exists(os.path.join(ckpt_dir, 'checkpoint')) and load:\n","  model.load_weights(checkpoint)\n","\n","\n","if train:\n","  model.fit(x=train_dataset,\n","            epochs=epochs,\n","            steps_per_epoch=len(dataset)/bs,\n","            validation_data=valid_dataset,\n","            #initial_epoch=7,\n","            validation_steps=len(dataset_valid),\n","            callbacks=callbacks)\n","  \n","  # Reload the weights to load the best checkpoint\n","  model.load_weights(checkpoint)\n","\n","  # Save model as h5\n","  #model_file = os.path.join(exp_dir, test_name + '.h5')\n","  #model.save(model_file, include_optimizer=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer TransformerEncoder has arguments in `__init__` and therefore must override `get_config`.\n","Epoch 1/100\n","2941/2941 [==============================] - 2738s 917ms/step - loss: 7.8244 - accuracy: 0.2900 - val_loss: 2.7476 - val_accuracy: 0.3172\n","Epoch 2/100\n","2941/2941 [==============================] - 2621s 891ms/step - loss: 2.7212 - accuracy: 0.3072 - val_loss: 2.7395 - val_accuracy: 0.3172\n","Epoch 3/100\n","2941/2941 [==============================] - 2632s 895ms/step - loss: 2.7163 - accuracy: 0.3083 - val_loss: 2.7352 - val_accuracy: 0.3172\n","Epoch 4/100\n","2941/2941 [==============================] - 2594s 882ms/step - loss: 2.7139 - accuracy: 0.3084 - val_loss: 2.7332 - val_accuracy: 0.3172\n","Epoch 5/100\n","1629/2941 [===============>..............] - ETA: 13:43 - loss: 2.7156 - accuracy: 0.3059"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-a4d21a2c09a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m#initial_epoch=7,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             callbacks=callbacks)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Reload the weights to load the best checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"yKtCtnG2qL_S"},"source":["# Prediction"]},{"cell_type":"code","metadata":{"id":"-TGi3cxn4i6r"},"source":["import json\n","\n","# Image data set generator and path\n","test_img_data_gen = ImageDataGenerator(rescale=1./255)\n","test_filepath = os.path.join(dataset_dir, 'test_questions.json')\n","\n","# Load test dictionary\n","with open(test_filepath, 'r') as f:\n","  test_items = json.load(f)\n","\n","# Initialize submission dict\n","submission_dict = {}\n","\n","for key, item in test_items.items():\n","  # Open test image\n","  img = Image.open(os.path.join(dataset_dir, 'Images', item['image_id'] + '.png'))\n","\n","  # Convert image to RGB and resize\n","  img = img.convert('RGB')\n","  img = img.resize((img_w, img_h))\n","  img_arr = np.array(img)\n","\n","  # Apply test transformation\n","  img_t = test_img_data_gen.get_random_transform(img_arr.shape, seed=SEED)\n","  img_arr = np.float32(test_img_data_gen.apply_transform(img_arr, img_t))\n","\n","  # Tokenize question\n","  question = item['question']\n","  tokenized_question = tokenizer.texts_to_sequences([question])\n","  question_input = pad_sequences(tokenized_question, maxlen=max_question_length, padding='post')\n","\n","  # Prediction\n","  out_sigmoid = model.predict([question_input, np.expand_dims(img_arr, axis=0)])[0]\n","\n","  prediction = tf.argmax(out_sigmoid, -1).numpy()\n","\n","  submission_dict[key] = prediction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3ESdlE_hHzd"},"source":["import os\n","\n","csv_fname = 'submission.csv'\n","\n","# Write submission dict as csv file\n","with open(os.path.join('/content/drive/MyDrive/AN2DL', csv_fname), 'w') as f:\n","\n","    f.write('Id,Category\\n')\n","\n","    for key, value in submission_dict.items():\n","        f.write(key + ',' + str(value) + '\\n')"],"execution_count":null,"outputs":[]}]}