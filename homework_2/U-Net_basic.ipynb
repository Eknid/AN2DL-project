{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"U-Net_basic.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mginZGe4lt3C"},"source":["# Data preparation"]},{"cell_type":"code","metadata":{"id":"nbWLUxZwZV6l"},"source":["import os\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import keras\n","import json\n","\n","SEED = 1234\n","tf.random.set_seed(SEED)\n","\n","subset_folder='Bipbip'\n","\n","subset_file=subset_folder.replace('/','_')+'.json'\n","\n","test_name = 'U-Net_basic' + subset_folder.replace('/','_')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"656CZGn7VtVE"},"source":["# Set the base directory for Colab and non Colab environment\n","if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  base_dir = '/content/drive/My Drive/AN2DL/homework_2'\n","else:\n","  base_dir = os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGMuBoZ39Hn-"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Create two different training ImageDataGenerator object for images and corresponding masks\n","img_data_gen = ImageDataGenerator(rotation_range=10,\n","                                  width_shift_range=10,\n","                                  height_shift_range=10,\n","                                  zoom_range=0.3,\n","                                  horizontal_flip=True,\n","                                  vertical_flip=True,\n","                                  fill_mode='reflect',\n","                                  rescale=1./255)\n","mask_data_gen = ImageDataGenerator(rotation_range=10,\n","                                   width_shift_range=10,\n","                                   height_shift_range=10,\n","                                   zoom_range=0.3,\n","                                   horizontal_flip=True,\n","                                   vertical_flip=True,\n","                                   fill_mode='reflect')\n","\n","# Create validation and test ImageDataGenerator objects\n","valid_img_data_gen = ImageDataGenerator(rescale=1./255)\n","valid_mask_data_gen = ImageDataGenerator()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7mWSFGRZdNG"},"source":["from PIL import Image\n","\n","class CustomDataset(tf.keras.utils.Sequence):\n","  \n","  RGB_TO_TARGET = [([0, 0, 0], 0),        # background\n","                   ([216, 124, 18], 0),   # background\n","                   ([255, 255, 255], 1),  # crop\n","                   ([216, 67, 82], 2)]    # weed\n","\n","  def __init__(self, dataset_dir, which_subset, subset_file, subset_folder,\n","               img_generator=None, mask_generator=None, preprocessing_function=None, out_shape=None):\n","    if which_subset == 'training':\n","      subset_dir = os.path.join(dataset_dir, 'Training', subset_folder)\n","    elif which_subset == 'validation':\n","      subset_dir = os.path.join(dataset_dir, 'Validation', subset_folder)\n","\n","    subset_file = os.path.join(subset_dir, subset_file)\n","    \n","    with open(subset_file, 'r') as f:\n","      files = json.load(f)\n","    \n","\n","    self.files = files\n","    self.which_subset = which_subset\n","    self.dataset_dir = dataset_dir\n","    self.subset_dir = subset_dir\n","    self.subset_file = subset_file\n","    self.img_generator = img_generator\n","    self.mask_generator = mask_generator\n","    self.preprocessing_function = preprocessing_function\n","    self.out_shape = out_shape\n","\n","  def __len__(self):\n","    return len(self.files)\n","\n","  def __getitem__(self, index):\n","    # Read Image\n","    img = Image.open(os.path.join(self.subset_dir, \n","                                  self.files[index]['path'], \n","                                  'Images', self.files[index]['file'] + '.jpg'))\n","    mask = Image.open(os.path.join(self.subset_dir, \n","                                   self.files[index]['path'], \n","                                   'Masks', self.files[index]['file'] + '.png'))\n","\n","    # Resize image and mask\n","    if self.out_shape is not None:\n","      img = img.resize(self.out_shape)\n","      mask = mask.resize(self.out_shape, resample=Image.NEAREST)\n","\n","    img_arr = np.array(img)\n","    mask_arr = np.array(mask)\n","\n","    if self.img_generator is not None and self.mask_generator is not None:\n","      # Perform data augmentation\n","      # We can get a random transformation from the ImageDataGenerator using get_random_transform\n","      # and we can apply it to the image using apply_transform\n","      img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n","      mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n","      img_arr = self.img_generator.apply_transform(img_arr, img_t)\n","      # ImageDataGenerator use bilinear interpolation for augmenting the images.\n","      # Thus, when applied to the masks it will output 'interpolated classes', which\n","      # is an unwanted behaviour. As a trick, we can transform each class mask \n","      # separately and then we can cast to integer values (as in the binary segmentation notebook).\n","      # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n","      out_mask = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n","\n","      for rgb, c in self.RGB_TO_TARGET:\n","        if c > 0:\n","          curr_class_arr = np.copy(mask_arr)\n","          curr_class_arr[np.where(np.all(curr_class_arr != rgb, axis=-1))] = [0, 0, 0]\n","          curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n","\n","          class_out_mask = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n","          class_out_mask[np.where(np.all(curr_class_arr == rgb, axis=-1))] = c\n","\n","          out_mask += class_out_mask\n","    else:\n","      out_mask = mask_arr\n","    \n","    if self.preprocessing_function is not None:\n","        img_arr = self.preprocessing_function(img_arr)\n","\n","    return img_arr, np.float32(out_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QolQ3xviAaGy"},"source":["img_h = 1536\n","img_w = 2048\n","\n","dataset_dir = os.path.join(base_dir, 'Development_Dataset')\n","\n","dataset = CustomDataset(dataset_dir, 'training', \n","                        subset_folder=subset_folder,\n","                        subset_file=subset_file,\n","                        img_generator=img_data_gen, \n","                        mask_generator=mask_data_gen, \n","                        out_shape=(img_w, img_h))\n","\n","dataset_valid = CustomDataset(dataset_dir, 'validation',\n","                              subset_folder = 'Bipbip',\n","                              subset_file='Bipbip.json',\n","                              img_generator=valid_img_data_gen, \n","                              mask_generator=valid_mask_data_gen, \n","                              out_shape=(img_w, img_h))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rk14AY2SBVdf"},"source":["train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w]))\n","\n","train_dataset = train_dataset.batch(1)\n","\n","train_dataset = train_dataset.repeat()\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([img_h, img_w, 3], [img_h, img_w]))\n","valid_dataset = valid_dataset.batch(1)\n","\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hrPEc-5Pntp2"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"m0UqA8383qyN"},"source":["def conv2D(x, filters, k_size=3, strirdes=1, relu=0, pool_size=0):\n","    x = tf.keras.layers.Conv2D(filters=filters, \n","                               kernel_size=k_size,\n","                               strides=strirdes,\n","                               padding='same',\n","                               input_shape=[None])(x)\n","    if relu:\n","        x = tf.keras.layers.ReLU()(x)\n","    if pool_size:\n","        x = tf.keras.layers.MaxPool2D(pool_size=pool_size)(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xN41PO3rX27I"},"source":["def upSampling(x, up_type=0, up_size=2, filters=1):\n","    if up_type == 1:\n","        x = tf.keras.layers.Conv2DTranspose(filters=filters, \n","                                            kernel_size=3,\n","                                            strides=up_size,\n","                                            padding='same',\n","                                            input_shape=[None])(x)\n","    else:\n","        x = tf.keras.layers.UpSampling2D(up_size,\n","                                         interpolation='bilinear')(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MkqjYNNiBJT"},"source":["def cropping2D(y, x):\n","    h_cropp = y.shape[1]-x.shape[1]\n","    w_cropp = y.shape[2]-x.shape[2]\n","    if h_cropp%2:\n","        h_cropp = (h_cropp//2+1, h_cropp//2)\n","    else:\n","        h_cropp = (h_cropp//2, h_cropp//2)\n","    if w_cropp%2:\n","        w_cropp = (w_cropp//2+1, w_cropp//2)\n","    else:\n","        w_cropp = (w_cropp//2, w_cropp//2)\n","\n","    y = tf.keras.layers.Cropping2D(cropping=(h_cropp, w_cropp))(y)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aKKMLisCbxX"},"source":["def get_encoder(inputs, deepth, filters):\n","    skip_passes = []\n","\n","    x = inputs\n","    ### [First half of the network: downsampling inputs] ###\n","    for i in range(deepth):\n","        k_size = 3\n","        x = conv2D(x, filters, k_size=k_size, relu=1)\n","        x = conv2D(x, filters, k_size=k_size, relu=1)\n","        skip_passes.append(x)\n","        x = tf.keras.layers.MaxPool2D(pool_size=2)(x)\n","        filters *= 2\n","\n","    return x, skip_passes, filters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDdn3yvnlriL"},"source":["def get_UNet(input_shape, num_classe):\n","    inputs = keras.Input(shape=input_shape)\n","\n","    deepth = 4\n","    filters = 32\n","\n","    x, skip_passes, filters = get_encoder(inputs, deepth, filters)\n","    \n","    skip_passes = skip_passes[::-1]\n","    # Bottleneck\n","    x = conv2D(x, filters, relu=1)\n","    x = conv2D(x, filters, relu=1)\n","\n","    ### Decoder ###    \n","    for i in range(deepth):\n","        filters //= 2\n","        x = upSampling(x, filters)\n","        y = skip_passes[i]\n","        x = tf.keras.layers.concatenate([y, x]) \n","        x = conv2D(x, filters, relu=1)\n","        x = conv2D(x, filters, relu=1)\n","\n","    # Add a per-pixel classification layer\n","    outputs = tf.keras.layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n","\n","    # Define the model\n","    model = tf.keras.Model(inputs, outputs)\n","    return model\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6t2k_jaB9Kz"},"source":["img_size = (img_h, img_w, 3)\n","num_classes = 3\n","\n","# Free up RAM in case the model definition cells were run multiple times\n","keras.backend.clear_session()\n","\n","model = get_UNet(img_size, num_classes)\n","\n","# Visualize created model as a table\n","model.summary()\n","\n","# Visualize initialized weights\n","# model.weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnnXNjSal_en"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \n","# learning rate\n","lr = 1e-3\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","epochs = 100\n","\n","# Here we define the intersection over union for each class in the batch.\n","# Then we compute the final iou as the mean over classes\n","def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.argmax(y_pred, -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","# Validation metrics\n","# ------------------\n","metrics = [meanIoU]\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ck6esfPtmNYP"},"source":["exp_dir = os.path.join(base_dir, test_name)\n","if not os.path.exists(exp_dir):\n","  os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Checkpoint callback, generate a checkpoint at each epoch\n","# There is only one checkpoint overwritten only if the new one has a lower validation loss\n","ckpt_dir = os.path.join(exp_dir, 'checkpoints')\n","if not os.path.exists(ckpt_dir):\n","  os.makedirs(ckpt_dir)\n","checkpoint = os.path.join(ckpt_dir, 'checkpoint_' + test_name + '.ckpt')\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint,\n","                                                   save_weights_only=True,\n","                                                   monitor='val_meanIoU',\n","                                                   mode='max',\n","                                                   save_best_only=True)\n","callbacks.append(ckpt_callback)\n","\n","# Checkpoint for tensorboard logs\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","  os.makedirs(tb_dir)\n","    \n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)\n","callbacks.append(tb_callback)\n","\n","# Checkpoint for early stopping in order to optimize the number of epochs\n","es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_meanIoU', mode='max', patience=5)\n","callbacks.append(es_callback)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPkEUjArmH4a"},"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/drive/My\\ Drive/AN2DL/homework_2/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"04N_zH9Xmzdn"},"source":["# If there exists a checkpoint it's loaded, otherwise the model is trained\n","# If train is True and there exists a checkpoint the trainig continues from it\n","\n","train = False\n","load = False\n","\n","if os.path.exists(os.path.join(ckpt_dir, 'checkpoint')) and load:\n","  model.load_weights(checkpoint)\n","else:\n","  train = True\n","\n","if train:\n","  model.fit(x=train_dataset,\n","            epochs=epochs,\n","            steps_per_epoch=len(dataset),\n","            validation_data=valid_dataset,\n","            validation_steps=len(dataset_valid), \n","            callbacks=callbacks)\n","  \n","  # Reload the weights to load the best checkpoint\n","  model.load_weights(checkpoint)\n","\n","  # Save model\n","  model_file = os.path.join(exp_dir, test_name + '.h5')\n","  model.save(model_file, include_optimizer=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yKtCtnG2qL_S"},"source":["# Prediction"]},{"cell_type":"code","metadata":{"id":"hJjjksCBnE5e"},"source":["import time\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","\n","%matplotlib inline\n","\n","iterator = iter(valid_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxWwFmIvnLOE"},"source":["fig, ax = plt.subplots(1, 3, figsize=(10, 30))\n","fig.show()\n","image, target = next(iterator)\n","\n","target = target[0]\n","out_sigmoid = model.predict(image)\n","image = image[0]\n","prediction = tf.argmax(out_sigmoid[0], -1)\n","\n","# Assign colors (just for visualization)\n","target_img = np.zeros([target.shape[0], target.shape[1], 3])\n","prediction_img = np.zeros([prediction.shape[0], prediction.shape[1], 3])\n","\n","target_img[np.where(target == 0)] = [0, 0, 0]\n","target_img[np.where(target == 1)] = [0, 255, 0]\n","target_img[np.where(target == 2)] = [255, 0, 0]\n","\n","prediction_img[np.where(prediction == 0)] = [0, 0, 0]\n","prediction_img[np.where(prediction == 1)] = [0, 255, 0]\n","prediction_img[np.where(prediction == 2)] = [255, 0, 0]\n","\n","ax[0].imshow(np.uint8(image))\n","ax[1].imshow(np.uint8(target_img))\n","ax[2].imshow(np.uint8(prediction_img))\n","\n","fig.canvas.draw()\n","time.sleep(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wykqw9xp3mWG"},"source":["def rle_encode(img):\n","  \n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n","\n","\n","def add_prediction(img_name, img_shape, mask_arr, team, crop, submission_dict):\n","\n","  mask_arr = np.array(mask_arr)\n","  submission_dict[img_name] = {}\n","  submission_dict[img_name]['shape'] = img_shape\n","  submission_dict[img_name]['team'] = team\n","  submission_dict[img_name]['crop'] = crop\n","  submission_dict[img_name]['segmentation'] = {}\n","\n","  # RLE encoding\n","  # crop\n","  rle_encoded_crop = rle_encode(mask_arr == 1)\n","  # weed\n","  rle_encoded_weed = rle_encode(mask_arr == 2)\n","\n","  submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n","  submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n","\n","  return submission_dict\n","\n","  # Please notice that in this example we have a single prediction.\n","  # For the competition you have to provide segmentation for each of\n","  # the test images.\n","\n","  # Finally, save the results into the submission.json file\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-TGi3cxn4i6r"},"source":["import json\n","import cv2\n","\n","test_img_data_gen = ImageDataGenerator(rescale=1./255)\n","test_dir = os.path.join(dataset_dir, 'Test_Dev')\n","submission_dict = {}\n","\n","for team in os.listdir(test_dir):\n","  team_test_dir = os.path.join(test_dir, team)\n","\n","  for crop in os.listdir(team_test_dir):\n","    crop_test_dir = os.path.join(team_test_dir, crop)\n","    img_test_dir = os.path.join(crop_test_dir, 'Images')\n","\n","    for file in os.listdir(img_test_dir):\n","      img = Image.open(os.path.join(img_test_dir, file))\n","      img_shape = img.size\n","      img = img.resize((1024, 768), Image.LANCZOS)\n","      img_arr = np.array(img)\n","      img_t = test_img_data_gen.get_random_transform(img_arr.shape, seed=SEED)\n","      img_arr = test_img_data_gen.apply_transform(img_arr, img_t)\n","\n","      # Prediction of the images from the test generator, the label for each class is the argmax of the prediction\n","      out_sigmoid = model.predict(np.expand_dims(img_arr, axis=0))[0]\n","      out_sigmoid = cv2.resize(out_sigmoid, img_shape, interpolation=cv2.INTER_CUBIC)\n","\n","      prediction_mask = tf.argmax(out_sigmoid, -1)\n","      img_name = os.path.splitext(file)[0]\n","\n","      submission_dict = add_prediction(img_name, img_shape, prediction_mask, team, crop, submission_dict)\n","\n","with open(os.path.join(base_dir, exp_dir, 'submission.json'), 'w') as f:\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]}]}